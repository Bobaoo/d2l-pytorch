{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 参数管理\n",
    "\n",
    "一旦我们选择了架构并设置了超参数，我们就进入了训练阶段。此时，我们的目标是找到使损失函数最小化的参数值。经过训练后，我们将需要使用这些参数来做出未来的预测。此外，有时我们希望提取参数，以便在其他环境中复用它们，将模型保存到磁盘，以便它可以在其他软件中执行，或者为了获得科学的理解而进行检查。\n",
    "\n",
    "大多数情况下，我们可以忽略声明和操作参数的具体细节，而只依靠深度学习框架来完成繁重的工作。然而，当我们离开具有标准层的层叠架构时，我们有时会陷入声明和操作参数的麻烦中。在本节中，我们将介绍以下内容：\n",
    "\n",
    "* 访问参数，用于调试、诊断和可视化。\n",
    "* 参数初始化。\n",
    "* 在不同模型组件间共享参数。\n",
    "\n",
    "(**我们首先关注具有单隐藏层的多层感知机。**)\n"
   ],
   "metadata": {
    "origin_pos": 0
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8,1))\n",
    "X = torch.rand((2,4))\n",
    "net(X)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.8345],\n",
       "        [0.6063]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [**参数访问**]\n",
    "\n",
    "我们从已有模型中访问参数。当通过`Sequential`类定义模型时，我们可以通过索引来访问模型的任意层。这就像模型是一个列表一样。每层的参数都在其属性中。如下所示，我们可以检查第二个全连接层的参数。\n"
   ],
   "metadata": {
    "origin_pos": 4
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "print(net[2].state_dict())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([('weight', tensor([[-0.1830, -0.0986,  0.1167, -0.1865,  0.2462,  0.2930,  0.2089, -0.0376]])), ('bias', tensor([0.1767]))])\n"
     ]
    }
   ],
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "输出的结果告诉我们一些重要的事情。首先，这个全连接层包含两个参数，分别是该层的权重和偏置。两者都存储为单精度浮点数（float32）。注意，参数名称允许我们唯一地标识每个参数，即使在包含数百个层的网络中也是如此。\n",
    "\n",
    "### [**目标参数**]\n",
    "\n",
    "注意，每个参数都表示为参数（parameter）类的一个实例。要对参数执行任何操作，首先我们需要访问底层的数值。有几种方法可以做到这一点。有些比较简单，而另一些则比较通用。下面的代码从第二个神经网络层提取偏置，提取后返回的是一个参数类实例，并进一步访问该参数的值。\n"
   ],
   "metadata": {
    "origin_pos": 8
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.1767], requires_grad=True)\n",
      "tensor([0.1767])\n"
     ]
    }
   ],
   "metadata": {
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "参数是复合的对象，包含值、梯度和额外信息。这就是我们需要显式请求值的原因。\n",
    "\n",
    "除了值之外，我们还可以访问每个参数的梯度。由于我们还没有调用这个网络的反向传播，所以参数的梯度处于初始状态。\n"
   ],
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "net[2].weight.grad == None"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### [**一次性访问所有参数**]\n",
    "\n",
    "当我们需要对所有参数执行操作时，逐个访问它们可能会很麻烦。当我们处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂，因为我们需要递归整个树来提取每个子块的参数。下面，我们将通过演示来比较访问第一个全连接层的参数和访问所有层。\n"
   ],
   "metadata": {
    "origin_pos": 15
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()])\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这为我们提供了另一种访问网络参数的方式，如下所示。\n"
   ],
   "metadata": {
    "origin_pos": 19
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "net.state_dict()['2.bias'].data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.1767])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {
    "origin_pos": 21,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### [**从嵌套块收集参数**]\n",
    "\n",
    "让我们看看，如果我们将多个块相互嵌套，参数命名约定是如何工作的。为此，我们首先定义一个生成块的函数（可以说是块工厂），然后将这些块组合到更大的块中。\n"
   ],
   "metadata": {
    "origin_pos": 23
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8 ,4), nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4):\n",
    "        # 在这里嵌套\n",
    "        net.add_module(f'block {i}', block1())\n",
    "    return net\n",
    "rgnet = nn.Sequential(block2(), nn.Linear(4, 1))\n",
    "rgnet(X)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.1027],\n",
       "        [0.1026]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {
    "origin_pos": 25,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在[**我们已经设计了网络，让我们看看它是如何组织的。**]\n"
   ],
   "metadata": {
    "origin_pos": 27
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print(rgnet)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {
    "origin_pos": 29,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "因为层是分层嵌套的，所以我们也可以像通过嵌套列表索引一样访问它们。例如，我们下面访问第一个主要的块，其中第二个子块的第一层的偏置项。\n"
   ],
   "metadata": {
    "origin_pos": 31
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "rgnet[0][1][0].bias.data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 0.0833,  0.0104,  0.2356,  0.3823, -0.2953,  0.0840,  0.0729,  0.3540])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {
    "origin_pos": 33,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 参数初始化\n",
    "\n",
    "我们知道了如何访问参数，现在让我们看看如何正确地初始化参数。我们在 :numref:`sec_numerical_stability` 中讨论了良好初始化的必要性。深度学习框架提供默认随机初始化。然而，我们经常希望根据其他规则初始化权重。深度学习框架提供了最常用的规则，也允许创建自定义初始化方法。\n"
   ],
   "metadata": {
    "origin_pos": 35
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵，这个范围是根据输入和输出维度计算出的。PyTorch的`nn.init`模块提供了多种预置初始化方法。\n"
   ],
   "metadata": {
    "origin_pos": 37,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### [**内置初始化**]\n",
    "\n",
    "让我们首先调用内置的初始化器。下面的代码将所有权重参数初始化为标准差为0.01的高斯随机变量，且将偏置参数设置为0。\n"
   ],
   "metadata": {
    "origin_pos": 39
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([ 0.0071, -0.0058,  0.0072, -0.0039]), tensor(0.))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {
    "origin_pos": 41,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们还可以将所有参数初始化为给定的常数（比如1）。\n"
   ],
   "metadata": {
    "origin_pos": 43
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data, net[0].bias.data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {
    "origin_pos": 45,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们还可以[**对某些块应用不同的初始化方法**]。例如，下面我们使用Xavier初始化方法初始化第一层，然后第二层初始化为常量值42。\n"
   ],
   "metadata": {
    "origin_pos": 47
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "def init_42(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 42)\n",
    "\n",
    "net[0].apply(xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 0.1041, -0.1352,  0.6947, -0.6161])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "metadata": {
    "origin_pos": 49,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### [**自定义初始化**]\n",
    "\n",
    "有时，深度学习框架没有提供我们需要的初始化方法。在下面的例子中，我们使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ with probability } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ with probability } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ with probability } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ],
   "metadata": {
    "origin_pos": 51
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "同样，我们实现了一个`my_init`函数来应用到`net`。\n"
   ],
   "metadata": {
    "origin_pos": 53,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape) for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Init weight torch.Size([8, 4])\n",
      "Init weight torch.Size([1, 8])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.0000,  0.0000, -0.0000,  7.2767],\n",
       "        [-8.9406, -0.0000,  0.0000,  5.7325]], grad_fn=<SliceBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {
    "origin_pos": 56,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "注意，我们始终可以直接设置参数。\n"
   ],
   "metadata": {
    "origin_pos": 58
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 43\n",
    "net[0].weight.data[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([43.0000,  1.0000,  1.0000,  8.2767])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {
    "origin_pos": 60,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## [**参数绑定**]\n",
    "\n",
    "有时我们希望在多个层间共享参数。让我们看看如何优雅地做这件事。在下面，我们定义一个稠密层，然后使用它的参数来设置另一个层的参数。\n"
   ],
   "metadata": {
    "origin_pos": 63
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数。\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(), \n",
    "                    shared, nn.ReLU(), \n",
    "                    nn.Linear(8, 1))\n",
    "net(X)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值。\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "metadata": {
    "origin_pos": 65,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这个例子表明第二层和第三层的参数是绑定的。它们不仅值相等，而且由相同的张量表示。因此，如果我们改变其中一个参数，另一个参数也会改变。你可能会想，当参数绑定时，梯度会发生什么情况？答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层和第三个隐藏层的梯度会加在一起。\n"
   ],
   "metadata": {
    "origin_pos": 67,
    "tab": [
     "pytorch"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 小结\n",
    "\n",
    "* 我们有几种方法可以访问、初始化和绑定模型参数。\n",
    "* 我们可以使用自定义初始化方法。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 使用 :numref:`sec_model_construction` 中定义的`FancyMLP`模型，访问各个层的参数。\n",
    "1. 查看初始化模块文档以了解不同的初始化方法。\n",
    "1. 构建包含共享参数层的多层感知机并对其进行训练。在训练过程中，观察模型各层的参数和梯度。\n",
    "1. 为什么共享参数是个好主意？\n"
   ],
   "metadata": {
    "origin_pos": 68
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/1829)\n"
   ],
   "metadata": {
    "origin_pos": 70,
    "tab": [
     "pytorch"
    ]
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('d2l': conda)"
  },
  "interpreter": {
   "hash": "44136080f06e37e99a37591ec7d86fe6a997caab45bf6984bee9c9718a57e31b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}